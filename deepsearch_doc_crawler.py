import time
import json
import re
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from typing import Dict, List, Optional
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DeepsearchDocCrawler:
    """Deepsearch API ë¬¸ì„œ ì „ë¬¸ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, output_dir: str = "deepsearch_docs"):
        self.driver = None
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # API ë¬¸ì„œ ì„¹ì…˜ êµ¬ì¡°
        self.sections = {
            "ì‹œìž‘í•˜ê¸°": "/api/#section/%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0",
            "API ì‚¬ìš©ë°©ë²•": "/api/#section/API-%EC%82%AC%EC%9A%A9%EB%B0%A9%EB%B2%95",
            "êµ­ë‚´ ê¸°ì‚¬": "/api/#section/%EA%B5%AD%EB%82%B4-%EA%B8%B0%EC%82%AC",
            "í•´ì™¸ ê¸°ì‚¬": "/api/#section/%ED%95%B4%EC%99%B8-%EA%B8%B0%EC%82%AC",
            "êµ­ë‚´ í† í”½": "/api/#section/%EA%B5%AD%EB%82%B4-%ED%86%A0%ED%94%BD",
            "í•´ì™¸ í† í”½": "/api/#section/%ED%95%B4%EC%99%B8-%ED%86%A0%ED%94%BD",
            "ë¸Œë¦¬í•‘": "/api/#section/%EB%B8%8C%EB%A6%AC%ED%95%91",
            "í•´ì™¸ ê³µì‹œ": "/api/#section/%ED%95%B4%EC%99%B8-%EA%B3%B5%EC%8B%9C",
            "êµ­ë‚´ ë¬¸ì„œ": "/api/#section/%EA%B5%AD%EB%82%B4-%EB%AC%B8%EC%84%9C"
        }
        
        self.base_url = "https://news.deepsearch.com"
    
    def setup_driver(self):
        """Selenium WebDriver ì„¤ì •"""
        chrome_options = Options()
        # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ë¹„í™œì„±í™” (ë””ë²„ê¹…ìš©)
        # chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--lang=ko-KR')
        
        # User-Agent ì„¤ì •
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        
        logger.info("âœ… Selenium WebDriver ì„¤ì • ì™„ë£Œ")
    
    def crawl_all_sections(self) -> Dict:
        """ëª¨ë“  API ë¬¸ì„œ ì„¹ì…˜ í¬ë¡¤ë§"""
        
        if not self.driver:
            self.setup_driver()
        
        all_docs = {
            "metadata": {
                "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "base_url": self.base_url,
                "total_sections": len(self.sections)
            },
            "sections": {}
        }
        
        logger.info(f"ðŸš€ {len(self.sections)}ê°œ ì„¹ì…˜ í¬ë¡¤ë§ ì‹œìž‘")
        
        for section_name, section_path in self.sections.items():
            logger.info(f"\nðŸ“„ [{section_name}] í¬ë¡¤ë§ ì¤‘...")
            
            try:
                section_data = self.crawl_section(section_name, section_path)
                all_docs["sections"][section_name] = section_data
                
                logger.info(f"âœ… [{section_name}] ì™„ë£Œ - {len(section_data.get('endpoints', []))}ê°œ ì—”ë“œí¬ì¸íŠ¸")
                
                # ê° ì„¹ì…˜ì„ ê°œë³„ íŒŒì¼ë¡œ ì €ìž¥
                self.save_section_to_file(section_name, section_data)
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"âŒ [{section_name}] í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                all_docs["sections"][section_name] = {"error": str(e)}
        
        # ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ ì €ìž¥
        self.save_all_docs(all_docs)
        
        # Markdown í˜•ì‹ìœ¼ë¡œë„ ì €ìž¥
        self.generate_markdown_docs(all_docs)
        
        logger.info(f"\nâœ… ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"ðŸ“‚ ì €ìž¥ ìœ„ì¹˜: {self.output_dir.absolute()}")
        
        return all_docs
    
    def crawl_section(self, section_name: str, section_path: str) -> Dict:
        """íŠ¹ì • ì„¹ì…˜ í¬ë¡¤ë§"""
        
        url = f"{self.base_url}{section_path}"
        
        try:
            # íŽ˜ì´ì§€ ë¡œë“œ
            self.driver.get(url)
            
            # JavaScript ë Œë”ë§ ëŒ€ê¸°
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì½˜í…ì¸  ë¡œë”©)
            time.sleep(5)
            
            # íŽ˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # ì„¹ì…˜ ë°ì´í„° ì¶”ì¶œ
            section_data = {
                "url": url,
                "title": section_name,
                "description": "",
                "endpoints": [],
                "examples": [],
                "parameters": {},
                "raw_content": ""
            }
            
            # ì½˜í…ì¸  ì¶”ì¶œ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì„ íƒìž ì‹œë„)
            content_selectors = [
                '.api-content',
                '.swagger-ui',
                '.documentation',
                'article',
                'main',
                '.content',
                '#api-docs'
            ]
            
            content_element = None
            for selector in content_selectors:
                try:
                    content_element = soup.select_one(selector)
                    if content_element:
                        break
                except:
                    continue
            
            if not content_element:
                # ì „ì²´ bodyì—ì„œ ì¶”ì¶œ
                content_element = soup.find('body')
            
            if content_element:
                # ì„¤ëª… ì¶”ì¶œ
                description = self._extract_description(content_element)
                section_data["description"] = description
                
                # API ì—”ë“œí¬ì¸íŠ¸ ì¶”ì¶œ
                endpoints = self._extract_endpoints(content_element)
                section_data["endpoints"] = endpoints
                
                # ì˜ˆì œ ì½”ë“œ ì¶”ì¶œ
                examples = self._extract_code_examples(content_element)
                section_data["examples"] = examples
                
                # íŒŒë¼ë¯¸í„° ì •ë³´ ì¶”ì¶œ
                parameters = self._extract_parameters(content_element)
                section_data["parameters"] = parameters
                
                # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                section_data["raw_content"] = content_element.get_text(separator="\n", strip=True)
            
            return section_data
            
        except Exception as e:
            logger.error(f"ì„¹ì…˜ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            raise
    
    def _extract_description(self, element) -> str:
        """ì„¹ì…˜ ì„¤ëª… ì¶”ì¶œ"""
        description = ""
        
        # ì„¤ëª… íŒ¨í„´ ì°¾ê¸°
        desc_selectors = [
            'p:first-of-type',
            '.description',
            '.intro',
            'h2 + p',
            'h1 + p'
        ]
        
        for selector in desc_selectors:
            desc_elem = element.select_one(selector)
            if desc_elem:
                description = desc_elem.get_text(strip=True)
                if len(description) > 20:  # ìœ íš¨í•œ ì„¤ëª…
                    break
        
        return description
    
    def _extract_endpoints(self, element) -> List[Dict]:
        """API ì—”ë“œí¬ì¸íŠ¸ ì¶”ì¶œ"""
        endpoints = []
        
        # HTTP ë©”ì„œë“œ íŒ¨í„´
        method_pattern = re.compile(r'(GET|POST|PUT|DELETE|PATCH)\s+(/[^\s]+)')
        
        # ì½”ë“œ ë¸”ë¡ì—ì„œ ì—”ë“œí¬ì¸íŠ¸ ì°¾ê¸°
        code_blocks = element.find_all(['code', 'pre'])
        
        for code in code_blocks:
            text = code.get_text()
            matches = method_pattern.findall(text)
            
            for method, path in matches:
                endpoint = {
                    "method": method,
                    "path": path,
                    "description": self._find_nearby_description(code),
                    "full_url": f"{self.base_url}{path}" if not path.startswith('http') else path
                }
                endpoints.append(endpoint)
        
        # í…Œì´ë¸”ì—ì„œ ì—”ë“œí¬ì¸íŠ¸ ì°¾ê¸°
        tables = element.find_all('table')
        for table in tables:
            rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
            
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 2:
                    # ì²« ë²ˆì§¸ ì…€ì— ë©”ì„œë“œ, ë‘ ë²ˆì§¸ ì…€ì— ê²½ë¡œ
                    method_text = cells[0].get_text(strip=True)
                    path_text = cells[1].get_text(strip=True)
                    
                    if any(m in method_text.upper() for m in ['GET', 'POST', 'PUT', 'DELETE']):
                        endpoint = {
                            "method": method_text.upper(),
                            "path": path_text,
                            "description": cells[2].get_text(strip=True) if len(cells) > 2 else "",
                            "full_url": f"{self.base_url}{path_text}" if not path_text.startswith('http') else path_text
                        }
                        endpoints.append(endpoint)
        
        # ì¤‘ë³µ ì œê±°
        unique_endpoints = []
        seen = set()
        for ep in endpoints:
            key = f"{ep['method']}:{ep['path']}"
            if key not in seen:
                seen.add(key)
                unique_endpoints.append(ep)
        
        return unique_endpoints
    
    def _extract_code_examples(self, element) -> List[Dict]:
        """ì˜ˆì œ ì½”ë“œ ì¶”ì¶œ"""
        examples = []
        
        # ì½”ë“œ ë¸”ë¡ ì°¾ê¸°
        code_blocks = element.find_all(['pre', 'code'])
        
        for i, code in enumerate(code_blocks):
            text = code.get_text(strip=True)
            
            if len(text) > 10:  # ìœ íš¨í•œ ì½”ë“œ
                # ì–¸ì–´ ê°ì§€
                language = "unknown"
                if 'curl' in text.lower() or text.startswith('curl'):
                    language = "bash"
                elif 'python' in text.lower() or 'import' in text or 'def ' in text:
                    language = "python"
                elif 'javascript' in text.lower() or 'const ' in text or 'function' in text:
                    language = "javascript"
                elif '{' in text and '"' in text:
                    language = "json"
                
                example = {
                    "id": f"example_{i+1}",
                    "language": language,
                    "code": text,
                    "description": self._find_nearby_description(code)
                }
                examples.append(example)
        
        return examples
    
    def _extract_parameters(self, element) -> Dict:
        """íŒŒë¼ë¯¸í„° ì •ë³´ ì¶”ì¶œ"""
        parameters = {}
        
        # í…Œì´ë¸”ì—ì„œ íŒŒë¼ë¯¸í„° ì°¾ê¸°
        tables = element.find_all('table')
        
        for table in tables:
            headers = [th.get_text(strip=True).lower() for th in table.find_all('th')]
            
            # íŒŒë¼ë¯¸í„° í…Œì´ë¸”ì¸ì§€ í™•ì¸
            if any(keyword in ' '.join(headers) for keyword in ['parameter', 'param', 'íŒŒë¼ë¯¸í„°', 'name', 'type']):
                rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
                
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) >= 2:
                        param_name = cells[0].get_text(strip=True)
                        param_type = cells[1].get_text(strip=True) if len(cells) > 1 else "string"
                        param_desc = cells[2].get_text(strip=True) if len(cells) > 2 else ""
                        param_required = "required" in row.get_text().lower()
                        
                        parameters[param_name] = {
                            "type": param_type,
                            "description": param_desc,
                            "required": param_required
                        }
        
        return parameters
    
    def _find_nearby_description(self, element) -> str:
        """ìš”ì†Œ ê·¼ì²˜ì˜ ì„¤ëª… í…ìŠ¤íŠ¸ ì°¾ê¸°"""
        description = ""
        
        # ì´ì „ í˜•ì œ ìš”ì†Œì—ì„œ ì„¤ëª… ì°¾ê¸°
        prev = element.find_previous(['p', 'div', 'span'])
        if prev:
            text = prev.get_text(strip=True)
            if len(text) > 10 and len(text) < 500:
                description = text
        
        return description
    
    def save_section_to_file(self, section_name: str, section_data: Dict):
        """ì„¹ì…˜ ë°ì´í„°ë¥¼ ê°œë³„ íŒŒì¼ë¡œ ì €ìž¥"""
        
        # íŒŒì¼ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ìž ì œê±°
        safe_name = re.sub(r'[^\w\s-]', '', section_name).strip().replace(' ', '_')
        
        # JSON ì €ìž¥
        json_path = self.output_dir / f"{safe_name}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(section_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"  ðŸ’¾ ì €ìž¥: {json_path.name}")
    
    def save_all_docs(self, all_docs: Dict):
        """ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ì €ìž¥"""
        
        json_path = self.output_dir / "deepsearch_api_complete.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(all_docs, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ðŸ“¦ ì „ì²´ ë¬¸ì„œ ì €ìž¥: {json_path.name}")
    
    def generate_markdown_docs(self, all_docs: Dict):
        """Markdown í˜•ì‹ìœ¼ë¡œ ë¬¸ì„œ ìƒì„±"""
        
        md_path = self.output_dir / "DEEPSEARCH_API_DOCS.md"
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write("# Deepsearch API ë¬¸ì„œ\n\n")
            f.write(f"**í¬ë¡¤ë§ ë‚ ì§œ**: {all_docs['metadata']['crawled_at']}\n\n")
            f.write("---\n\n")
            
            for section_name, section_data in all_docs['sections'].items():
                if 'error' in section_data:
                    f.write(f"## {section_name} âŒ\n\n")
                    f.write(f"ì˜¤ë¥˜: {section_data['error']}\n\n")
                    continue
                
                f.write(f"## {section_name}\n\n")
                
                # URL
                f.write(f"**URL**: {section_data.get('url', 'N/A')}\n\n")
                
                # ì„¤ëª…
                if section_data.get('description'):
                    f.write(f"### ðŸ“ ì„¤ëª…\n\n")
                    f.write(f"{section_data['description']}\n\n")
                
                # ì—”ë“œí¬ì¸íŠ¸
                if section_data.get('endpoints'):
                    f.write(f"### ðŸ”— API ì—”ë“œí¬ì¸íŠ¸\n\n")
                    for ep in section_data['endpoints']:
                        f.write(f"#### `{ep['method']}` {ep['path']}\n\n")
                        if ep.get('description'):
                            f.write(f"{ep['description']}\n\n")
                        f.write(f"**Full URL**: `{ep.get('full_url', 'N/A')}`\n\n")
                
                # íŒŒë¼ë¯¸í„°
                if section_data.get('parameters'):
                    f.write(f"### ðŸ“‹ íŒŒë¼ë¯¸í„°\n\n")
                    f.write("| íŒŒë¼ë¯¸í„° | íƒ€ìž… | í•„ìˆ˜ | ì„¤ëª… |\n")
                    f.write("|----------|------|------|------|\n")
                    for param_name, param_info in section_data['parameters'].items():
                        required = "âœ…" if param_info.get('required') else "âŒ"
                        f.write(f"| `{param_name}` | {param_info.get('type', 'string')} | {required} | {param_info.get('description', '')} |\n")
                    f.write("\n")
                
                # ì˜ˆì œ
                if section_data.get('examples'):
                    f.write(f"### ðŸ’» ì˜ˆì œ ì½”ë“œ\n\n")
                    for ex in section_data['examples'][:3]:  # ìµœëŒ€ 3ê°œ
                        if ex.get('description'):
                            f.write(f"**{ex['description']}**\n\n")
                        f.write(f"```{ex.get('language', 'bash')}\n")
                        f.write(f"{ex['code']}\n")
                        f.write("```\n\n")
                
                f.write("---\n\n")
        
        logger.info(f"ðŸ“„ Markdown ë¬¸ì„œ ìƒì„±: {md_path.name}")
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.driver:
            self.driver.quit()
            logger.info("ðŸ”’ WebDriver ì¢…ë£Œ")


# ========================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ========================================

def main():
    """Deepsearch API ë¬¸ì„œ í¬ë¡¤ë§ ì‹¤í–‰"""
    
    print("="*60)
    print("ðŸš€ Deepsearch API ë¬¸ì„œ í¬ë¡¤ëŸ¬")
    print("="*60)
    
    crawler = DeepsearchDocCrawler(output_dir="deepsearch_docs")
    
    try:
        # ëª¨ë“  ì„¹ì…˜ í¬ë¡¤ë§
        all_docs = crawler.crawl_all_sections()
        
        print("\n" + "="*60)
        print("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        print("="*60)
        print(f"\nðŸ“Š í†µê³„:")
        print(f"  - ì´ ì„¹ì…˜: {len(all_docs['sections'])}ê°œ")
        
        total_endpoints = sum(
            len(section.get('endpoints', []))
            for section in all_docs['sections'].values()
            if 'error' not in section
        )
        print(f"  - ì´ ì—”ë“œí¬ì¸íŠ¸: {total_endpoints}ê°œ")
        
        total_examples = sum(
            len(section.get('examples', []))
            for section in all_docs['sections'].values()
            if 'error' not in section
        )
        print(f"  - ì´ ì˜ˆì œ: {total_examples}ê°œ")
        
        print(f"\nðŸ“‚ ì €ìž¥ ìœ„ì¹˜: {crawler.output_dir.absolute()}")
        print("\níŒŒì¼ ëª©ë¡:")
        for file in sorted(crawler.output_dir.glob("*")):
            print(f"  - {file.name}")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        crawler.close()


if __name__ == "__main__":
    main()